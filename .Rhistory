filename.data = "data/UKR2308_MSNA_clean_data_230829_full.xlsx",      # the filename of your data for analysis
# filename.data.labeled = "data/???_labeled.xlsx",      # same as above, but ending with '_labeled'
filename.tool = "resources/MSNA_2023_Questionnaire_Final_CATI_cleaned.xlsx",
filename.daf.tabular = "resources/UKR_MSNA_MSNI_DAF_inters - Copy.xlsx", # filename of your kobo tool  # filename of your DAP
filename.labels.oblasts = 'resources/oblast_name.xlsx'
# filename.dap.visual = "resources/???Visual_DAP???.xlsx",      # filename of your visual DAP
)
params  <- c(
fix_sheet_names_to_match = "data",     # this should be one of "tool", "data", or "none"
combine_folder = "temp/combine/"
)
rmarkdown::render('analysis_tabular.Rmd',
output_file = paste0("output/", strings['dataset.name.short'], "Tabular_Analysis_", strings['out_date'],".html"))
gc()
knitr::opts_chunk$set(echo = TRUE)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
rm(list = ls())
directory_dictionary <- list(
research_cycle_name = 'xxxx',
round = 'xxxx',
dir.audits = "data/inputs/audits/reach/", # The directory to your audit files
dir.audits.check = "output/checking/audit/",# The directory to your audit summary files (you'll be checking these)
dir.requests = "output/checking/requests/", # the directory of your other_requests file
dir.responses = "output/checking/responses/", # the directory of your responses to open questions
enum_colname = "enum_id", # the column that contains the enumerator ID,
enum_comments = 'N_1_enumerator_comment_note', # the column that contains the enumerator's comments,
filename.tool = "resources/MSNA_2023_Questionnaire_Final_CATI_cleaned.xlsx", # the name of your Kobo tool and its path
data_name = "XXXX.xlsx", # the name of your dataframe
data_path = "data/inputs/kobo_export/", # the path to your dataframe
label_colname = 'label::English', # the name of your label column. Has to be identical in Kobo survey and choices sheets
dctime_short = "XXXX" # the data of your survey (just for naming)
)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
rm(list = ls())
directory_dictionary <- list(
research_cycle_name = 'xxxx',
round = 'xxxx',
dir.audits = "data/inputs/audits/reach/", # The directory to your audit files
dir.audits.check = "output/checking/audit/",# The directory to your audit summary files (you'll be checking these)
dir.requests = "output/checking/requests/", # the directory of your other_requests file
dir.responses = "output/checking/responses/", # the directory of your responses to open questions
enum_colname = "XXX", # the column that contains the enumerator ID,
enum_comments = 'XXX', # the column that contains the enumerator's comments,
filename.tool = "resources/XXX.xlsx", # the name of your Kobo tool and its path
data_name = "XXXX.xlsx", # the name of your dataframe
data_path = "data/inputs/kobo_export/", # the path to your dataframe
label_colname = 'label::English', # the name of your label column. Has to be identical in Kobo survey and choices sheets
dctime_short = "XXXX" # the data of your survey (just for naming)
)
api_key <- source('resources/microsoft.api.key_regional.R')$value
#-------------------------------Initialize packages, load tools -----------------------------
source("src/init.R")
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
rm(list = ls())
directory_dictionary <- list(
research_cycle_name = 'xxxx',
round = 'xxxx',
dir.audits = "data/inputs/audits/reach/", # The directory to your audit files
dir.audits.check = "output/checking/audit/",# The directory to your audit summary files (you'll be checking these)
dir.requests = "output/checking/requests/", # the directory of your other_requests file
dir.responses = "output/checking/responses/", # the directory of your responses to open questions
enum_colname = "enum_id", # the column that contains the enumerator ID,
enum_comments = 'N_1_enumerator_comment_note', # the column that contains the enumerator's comments,
filename.tool = "resources/MSNA_2023_Questionnaire_Final_CATI_cleaned.xlsx", # the name of your Kobo tool and its path
data_name = "XXXX.xlsx", # the name of your dataframe
data_path = "data/inputs/kobo_export/", # the path to your dataframe
label_colname = 'label::English', # the name of your label column. Has to be identical in Kobo survey and choices sheets
dctime_short = "XXXX" # the data of your survey (just for naming)
)
api_key <- source('resources/microsoft.api.key_regional.R')$value
#-------------------------------Initialize packages, load tools -----------------------------
source("src/init.R")
source("src/load_Data.R")
# load a single raw Kobo data export:
# and loads the data into kobo.raw.main, kobo.raw.loop1...
# also included are the standard steps of renaming uuid, and adding the loop_index
raw_data_filename <- list.files(directory_dictionary$data_path, full.names = T)
if(length(raw_data_filename) > 1) { stop("Found multiple files containing raw Kobo data! Please clean up the kobo_export folder.")
}else if(length(raw_data_filename) == 0){
warning("Raw Kobo data not found!")
kobo.raw.main <- data.frame()
kobo.raw.loop1 <- data.frame()
dataset_creation_time <- NA
dctime_short <- ""
}else if(length(raw_data_filename) == 1){
ls <- excel_sheets(path = raw_data_filename)
sheet_names <- if(length(ls)>1){
c('kobo.raw.main',paste0('kobo.raw.loop',1:(length(ls)-1)))
}else{
'kobo.raw.main'
}
for(i in 1:length(ls)){
if(i==1){
kobo.raw.main <- readxl::read_xlsx(raw_data_filename, col_types = "text", sheet = ls[i])
}else{
txt <- paste0(sheet_names[i],'=readxl::read_xlsx(raw_data_filename, col_types = "text", sheet = "',ls[i],'")%>%
mutate(loop_index = paste0("loop',i-1,'_", loop_index))'
)
eval(parse(text = txt))
}
}
dataset_creation_time <- as.Date(file.info(raw_data_filename)$ctime)
dctime_short <- str_extract(gsub('-', '', str_sub(dataset_creation_time, 3)), "\\d+")
}
rm(raw_data_filename)
list.files(directory_dictionary$data_path, full.names = T)
directory_dictionary$data_path
getwd()
setwd('C:/Users/reach/Desktop/Git/cleaning_template/')
list.files(directory_dictionary$data_path, full.names = T)
raw_data_filename <- list.files(directory_dictionary$data_path, full.names = T)
if(length(raw_data_filename) > 1) { stop("Found multiple files containing raw Kobo data! Please clean up the kobo_export folder.")
}else if(length(raw_data_filename) == 0){
warning("Raw Kobo data not found!")
kobo.raw.main <- data.frame()
kobo.raw.loop1 <- data.frame()
dataset_creation_time <- NA
dctime_short <- ""
}else if(length(raw_data_filename) == 1){
ls <- excel_sheets(path = raw_data_filename)
sheet_names <- if(length(ls)>1){
c('kobo.raw.main',paste0('kobo.raw.loop',1:(length(ls)-1)))
}else{
'kobo.raw.main'
}
for(i in 1:length(ls)){
if(i==1){
kobo.raw.main <- readxl::read_xlsx(raw_data_filename, col_types = "text", sheet = ls[i])
}else{
txt <- paste0(sheet_names[i],'=readxl::read_xlsx(raw_data_filename, col_types = "text", sheet = "',ls[i],'")%>%
mutate(loop_index = paste0("loop',i-1,'_", loop_index))'
)
eval(parse(text = txt))
}
}
dataset_creation_time <- as.Date(file.info(raw_data_filename)$ctime)
dctime_short <- str_extract(gsub('-', '', str_sub(dataset_creation_time, 3)), "\\d+")
}
source('src/sections/process_old_data.R')
# final preparation
# Rename your dataframes
raw.main <- kobo.raw.main
sheet_names <- sheet_names[sheet_names!='kobo.raw.main']
sheet_names_new <- gsub('kobo.','',sheet_names)
if(length(sheet_names_new)>0){
for(i in 1:length(sheet_names_new)){
txt <- paste0(sheet_names_new[i],' <- ',sheet_names[i])
eval(parse(text=txt))
}
}
# select the columns in your data that contain date elements
date_cols_main <- c("start","end", tool.survey %>% filter(type == "date" & datasheet == "main") %>% pull(name),
"submission_time") # add them here
# transform them into the datetime format
raw.main <- raw.main %>%
mutate_at(date_cols_main, ~ifelse(!str_detect(., '-'), as.character(convertToDateTime(as.numeric(.))), .))
rm(date_cols_main)
# If there were any changes in the tool during data collection, they can be run here
source('src/sections/tool_modification.R')
source('src/sections/section_1_remove_duplicates_no_consents.R')
min_duration_interview <- 5 # minimum duration of an interview (screen time in minutes)
max_duration_interview <- 60 # maximum duration of an interview (screen time in minutes)
pre_process_audit_files <- F # whether cases of respondent taking too long to answer 1 question should cleaned.
# if pre_process_audit_files =T, enter the maximum time that  the respondent can spend answering 1 question (in minutes)
max_length_answer_1_question <- 20
# Used during the check for soft duplicates.
# The minimum number of different columns that makes us confident that the entry is not a soft duplicate
min_num_diff_questions <- 8
print("Checking for soft duplicates in data grouped by enumerators...")
res.soft_duplicates <- utilityR::find.similar.surveys(raw.main, tool.survey, uuid = "uuid", enum.column=directory_dictionary$enum_colname)
analysis.result <- utilityR::analyse.similarity(res.soft_duplicates, enum.column=directory_dictionary$enum_colname, visualise=T,
boxplot.path="output/checking/audit/enumerators_surveys_")
analysis <- analysis.result$analysis
outliers <- analysis.result$outliers
analysis$data_id <- 'main'
outliers$data_id <- 'main'
soft.duplicates <- res.soft_duplicates %>%
filter(number_different_columns <= min_num_diff_questions) %>%
relocate(uuid, num_cols_not_NA,num_cols_idnk,`_id_most_similar_survey`,
number_different_columns) %>%
arrange(number_different_columns)
soft_dupl_list <- vector('list',length(ls))
soft_dupl_list[[1]] <- soft.duplicates
# same thing but for loops
if(length(sheet_names_new)>0){
for(i in 1:length(sheet_names_new)){
txt <- paste0(sheet_names_new[i], "_tmp <-" ,
sheet_names_new[i]," %>% left_join(raw.main %>% select(uuid, !!sym(directory_dictionary$enum_colname)))")
eval(parse(text = txt))
txt <- paste0('res.soft_duplicates_l <- utilityR::find.similar.surveys(
',sheet_names_new[i],'_tmp, tool.survey, uuid = "loop_index", enum.column=directory_dictionary$enum_colname, is.loop=T)')
eval(parse(text = txt))
analysis.result_l <- utilityR::analyse.similarity(res.soft_duplicates_l, enum.column=directory_dictionary$enum_colname, visualise=T,
boxplot.path=paste0("output/checking/audit/enumerators_surveys_",sheet_names_new[i]))
analysis_l <- analysis.result_l$analysis
outliers_l <- analysis.result_l$outliers
analysis_l$data_id <- sheet_names_new[i]
outliers_l$data_id <- sheet_names_new[i]
analysis <- rbind(analysis,analysis_l)
outliers <- rbind(outliers,outliers_l)
min_num_diff_questions_loop <- ceiling(min_num_diff_questions*(ncol(res.soft_duplicates_l)/ncol(res.soft_duplicates)))
soft.duplicates_l <- res.soft_duplicates_l %>%
filter(number_different_columns <= min_num_diff_questions_loop) %>%
relocate(uuid, loop_index, num_cols_not_NA,num_cols_idnk,`_id_most_similar_survey`,
number_different_columns) %>%
arrange(number_different_columns)
soft_dupl_list[[i+1]] <- soft.duplicates_l
txt <- paste0(sheet_names_new[i],'_tmp')
rm(txt)
}
}
names(soft_dupl_list) <- ls
write.xlsx(soft_dupl_list, make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates"))
write.xlsx(analysis, make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates_analysis"))
write.xlsx(outliers, make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates_outliers"))
cat("Check soft duplicates in soft.duplicates data frame or soft_duplicates xlsx file")
cat("Check outliers of enumerators surveys group in output/checking/outliers/enumerators_surveys_2sd.pdf")
cat("Also, you can find analysis of the enumerators in analysis data frame, and outliers in outliers data frame.
If you want to check data without analysis, res.soft_duplicates for you. You can do different manipulations by yourself")
rm(audits, data.audit, analysis.result)
# Enter uuids of the interviews that are soft duplicates to remove:
deletion.log.softduplicates <- data.frame()
deletion.log.softduplicates_loop <- data.frame()
for(i in 1:length(ls)){
if(i==1){
main_dupl <- readxl::read_xlsx(make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates"),
col_types = "text", sheet = ls[i])
ids <- c(main_dupl$uuid)
deletion.log.softduplicates <- utilityR::create.deletion.log(raw.main %>% filter(uuid %in% ids),
directory_dictionary$enum_colname, "Soft duplicate")
}else{
loop_dupl <- readxl::read_xlsx(make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates"),
col_types = "text", sheet = ls[i])
if( nrow(loop_dupl)>0){
unique_uuids <- length(unique(loop_dupl$uuid))
txt <- paste0('The uploaded soft duplicate check file from datasheet',ls[i],
' has ',unique_uuids,' unique uuids that are going to be removed. Are you sure this is correct?
Enter Y to continue')
res <- readline(txt)
if(res == 'Y'){
ids <- c(loop_dupl$loop_index)
txt <- paste0('loop_frame <-', sheet_names_new[i-1],' %>% filter(loop_index %in% ids)')
eval(parse(text = txt))
deletion.log.softduplicates_loop <- utilityR::create.deletion.log(loop_frame,
directory_dictionary$enum_colname, "Soft duplicate",
is.loop = T, data.main = raw.main)
rm('loop_frame')
deletion.log.softduplicates <- bind_rows(deletion.log.softduplicates, deletion.log.softduplicates_loop)
}
}
}
}
txt <- paste0('The uploaded soft duplicate check file from datasheet',ls[i],
' has ',unique_uuids,' unique uuids that are going to be removed. Are you sure this is correct? Enter Y to continue:')
# Enter uuids of the interviews that are soft duplicates to remove:
deletion.log.softduplicates <- data.frame()
deletion.log.softduplicates_loop <- data.frame()
for(i in 1:length(ls)){
if(i==1){
main_dupl <- readxl::read_xlsx(make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates"),
col_types = "text", sheet = ls[i])
ids <- c(main_dupl$uuid)
deletion.log.softduplicates <- utilityR::create.deletion.log(raw.main %>% filter(uuid %in% ids),
directory_dictionary$enum_colname, "Soft duplicate")
}else{
loop_dupl <- readxl::read_xlsx(make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates"),
col_types = "text", sheet = ls[i])
if( nrow(loop_dupl)>0){
unique_uuids <- length(unique(loop_dupl$uuid))
txt <- paste0('The uploaded soft duplicate check file from datasheet',ls[i],
' has ',unique_uuids,' unique uuids that are going to be removed. Are you sure this is correct? Enter Y to continue:')
res <- readline(txt)
if(res == 'Y'){
ids <- c(loop_dupl$loop_index)
txt <- paste0('loop_frame <-', sheet_names_new[i-1],' %>% filter(loop_index %in% ids)')
eval(parse(text = txt))
deletion.log.softduplicates_loop <- utilityR::create.deletion.log(loop_frame,
directory_dictionary$enum_colname, "Soft duplicate",
is.loop = T, data.main = raw.main)
rm('loop_frame')
deletion.log.softduplicates <- bind_rows(deletion.log.softduplicates, deletion.log.softduplicates_loop)
}
}
}
}
res
ids
deletion.log.softduplicates
i
ls[i]
View(deletion.log.softduplicates)
# Enter uuids of the interviews that are soft duplicates to remove:
deletion.log.softduplicates <- data.frame()
deletion.log.softduplicates_loop <- data.frame()
for(i in 1:length(ls)){
if(i==1){
main_dupl <- readxl::read_xlsx(make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates"),
col_types = "text", sheet = ls[i])
ids <- c(main_dupl$uuid)
deletion.log.softduplicates <- utilityR::create.deletion.log(raw.main %>% filter(uuid %in% ids),
directory_dictionary$enum_colname, "Soft duplicate")
}else{
loop_dupl <- readxl::read_xlsx(make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates"),
col_types = "text", sheet = ls[i])
if( nrow(loop_dupl)>0){
unique_uuids <- length(unique(loop_dupl$uuid))
txt <- paste0('The uploaded soft duplicate check file from datasheet',ls[i],
' has ',unique_uuids,' unique uuids that are going to be removed. Are you sure this is correct? Enter Y to continue:')
res <- readline(txt)
if(res == 'Y'){
ids <- c(loop_dupl$loop_index)
txt <- paste0('loop_frame <-', sheet_names_new[i-1],' %>% filter(loop_index %in% ids)')
eval(parse(text = txt))
deletion.log.softduplicates_loop <- utilityR::create.deletion.log(loop_frame,
directory_dictionary$enum_colname, "Soft duplicate",
is.loop = T, data.main = raw.main)
rm('loop_frame')
deletion.log.softduplicates <- bind_rows(deletion.log.softduplicates, deletion.log.softduplicates_loop)
}
}
}
}
# Enter uuids of the interviews that are soft duplicates to remove:
deletion.log.softduplicates <- data.frame()
deletion.log.softduplicates_loop <- data.frame()
for(i in 1:length(ls)){
if(i==1){
main_dupl <- readxl::read_xlsx(make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates"),
col_types = "text", sheet = ls[i])
ids <- c(main_dupl$uuid)
deletion.log.softduplicates <- utilityR::create.deletion.log(raw.main %>% filter(uuid %in% ids),
directory_dictionary$enum_colname, "Soft duplicate")
}else{
loop_dupl <- readxl::read_xlsx(make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates"),
col_types = "text", sheet = ls[i])
if( nrow(loop_dupl)>0){
unique_uuids <- length(unique(loop_dupl$uuid))
txt <- paste0('The uploaded soft duplicate check file from datasheet',ls[i],
' has ',unique_uuids,' unique uuids that are going to be removed. Are you sure this is correct? Enter Y to continue:')
res <- readline(txt)
if(res == 'Y'){
ids <- c(loop_dupl$loop_index)
txt <- paste0('loop_frame <-', sheet_names_new[i-1],' %>% filter(loop_index %in% ids)')
eval(parse(text = txt))
deletion.log.softduplicates_loop <- utilityR::create.deletion.log(loop_frame,
directory_dictionary$enum_colname, "Soft duplicate",
is.loop = T, data.main = raw.main)
rm('loop_frame')
rm('res')
deletion.log.softduplicates <- bind_rows(deletion.log.softduplicates, deletion.log.softduplicates_loop)
}
}
}
}
loop_dupl
# Enter uuids of the interviews that are soft duplicates to remove:
deletion.log.softduplicates <- data.frame()
deletion.log.softduplicates_loop <- data.frame()
for(i in 1:length(ls)){
if(i==1){
main_dupl <- readxl::read_xlsx(make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates"),
col_types = "text", sheet = ls[i])
ids <- c(main_dupl$uuid)
deletion.log.softduplicates <- utilityR::create.deletion.log(raw.main %>% filter(uuid %in% ids),
directory_dictionary$enum_colname, "Soft duplicate")
}else{
loop_dupl <- readxl::read_xlsx(make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates"),
col_types = "text", sheet = ls[i])
if( nrow(loop_dupl)>0){
unique_uuids <- length(unique(loop_dupl$uuid))
txt <- paste0('The uploaded soft duplicate check file from datasheet',ls[i],
' has ',unique_uuids,' unique uuids that are going to be removed. Are you sure this is correct? Enter Y to continue:')
res <- menu(c("Y", "n"), title=txt)
# res <- readline(txt)
if(res == 'Y'){
ids <- c(loop_dupl$loop_index)
txt <- paste0('loop_frame <-', sheet_names_new[i-1],' %>% filter(loop_index %in% ids)')
eval(parse(text = txt))
deletion.log.softduplicates_loop <- utilityR::create.deletion.log(loop_frame,
directory_dictionary$enum_colname, "Soft duplicate",
is.loop = T, data.main = raw.main)
rm('loop_frame')
rm('res')
deletion.log.softduplicates <- bind_rows(deletion.log.softduplicates, deletion.log.softduplicates_loop)
}
}
}
}
# Enter uuids of the interviews that are soft duplicates to remove:
deletion.log.softduplicates <- data.frame()
deletion.log.softduplicates_loop <- data.frame()
for(i in 1:length(ls)){
if(i==1){
main_dupl <- readxl::read_xlsx(make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates"),
col_types = "text", sheet = ls[i])
ids <- c(main_dupl$uuid)
deletion.log.softduplicates <- utilityR::create.deletion.log(raw.main %>% filter(uuid %in% ids),
directory_dictionary$enum_colname, "Soft duplicate")
}else{
loop_dupl <- readxl::read_xlsx(make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates"),
col_types = "text", sheet = ls[i])
if( nrow(loop_dupl)>0){
unique_uuids <- length(unique(loop_dupl$uuid))
txt <- paste0('The uploaded soft duplicate check file from datasheet',ls[i],
' has ',unique_uuids,' unique uuids that are going to be removed. \nAre you sure this is correct? Enter Y to continue:')
res <- menu(c("Y", "n"), title=txt)
# res <- readline(txt)
if(res == 'Y'){
ids <- c(loop_dupl$loop_index)
txt <- paste0('loop_frame <-', sheet_names_new[i-1],' %>% filter(loop_index %in% ids)')
eval(parse(text = txt))
deletion.log.softduplicates_loop <- utilityR::create.deletion.log(loop_frame,
directory_dictionary$enum_colname, "Soft duplicate",
is.loop = T, data.main = raw.main)
rm('loop_frame')
rm('res')
deletion.log.softduplicates <- bind_rows(deletion.log.softduplicates, deletion.log.softduplicates_loop)
}
}
}
}
deletion.log.softduplicates
# Enter uuids of the interviews that are soft duplicates to remove:
deletion.log.softduplicates <- data.frame()
deletion.log.softduplicates_loop <- data.frame()
for(i in 1:length(ls)){
if(i==1){
main_dupl <- readxl::read_xlsx(make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates"),
col_types = "text", sheet = ls[i])
ids <- c(main_dupl$uuid)
deletion.log.softduplicates <- utilityR::create.deletion.log(raw.main %>% filter(uuid %in% ids),
directory_dictionary$enum_colname, "Soft duplicate")
}else{
loop_dupl <- readxl::read_xlsx(make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates"),
col_types = "text", sheet = ls[i])
if( nrow(loop_dupl)>0){
unique_uuids <- length(unique(loop_dupl$uuid))
txt <- paste0('The uploaded soft duplicate check file from datasheet',ls[i],
' has ',unique_uuids,' unique uuids that are going to be removed. \nAre you sure this is correct? Enter Y to continue:')
res <- menu(c("Y", "n"), title=txt)
# res <- readline(txt)
if(res == 'Y'){
ids <- c(loop_dupl$loop_index)
txt <- paste0('loop_frame <-', sheet_names_new[i-1],' %>% filter(loop_index %in% ids)')
eval(parse(text = txt))
deletion.log.softduplicates_loop <- utilityR::create.deletion.log(loop_frame,
directory_dictionary$enum_colname, "Soft duplicate",
is.loop = T, data.main = raw.main)
rm('loop_frame')
rm('res')
deletion.log.softduplicates <- bind_rows(deletion.log.softduplicates, deletion.log.softduplicates_loop)
}
}
}
}
deletion.log.softduplicates
res
res == 1
# Enter uuids of the interviews that are soft duplicates to remove:
deletion.log.softduplicates <- data.frame()
deletion.log.softduplicates_loop <- data.frame()
for(i in 1:length(ls)){
if(i==1){
main_dupl <- readxl::read_xlsx(make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates"),
col_types = "text", sheet = ls[i])
ids <- c(main_dupl$uuid)
deletion.log.softduplicates <- utilityR::create.deletion.log(raw.main %>% filter(uuid %in% ids),
directory_dictionary$enum_colname, "Soft duplicate")
}else{
loop_dupl <- readxl::read_xlsx(make.filename.xlsx(directory_dictionary$dir.audits.check, "soft_duplicates"),
col_types = "text", sheet = ls[i])
if( nrow(loop_dupl)>0){
unique_uuids <- length(unique(loop_dupl$uuid))
txt <- paste0('The uploaded soft duplicate check file from datasheet',ls[i],
' has ',unique_uuids,' unique uuids that are going to be removed. \nAre you sure this is correct? Enter Y to continue:')
res <- menu(c("Y", "n"), title=txt)
# res <- readline(txt)
if(res == 1){
ids <- c(loop_dupl$loop_index)
txt <- paste0('loop_frame <-', sheet_names_new[i-1],' %>% filter(loop_index %in% ids)')
eval(parse(text = txt))
deletion.log.softduplicates_loop <- utilityR::create.deletion.log(loop_frame,
directory_dictionary$enum_colname, "Soft duplicate",
is.loop = T, data.main = raw.main)
rm('loop_frame')
rm('res')
deletion.log.softduplicates <- bind_rows(deletion.log.softduplicates, deletion.log.softduplicates_loop)
}
}
}
}
deletion.log.softduplicates
install.packages("translator")
library(translateR)
help(MicrosoftTranslatorAuth)
??MicrosoftTranslatorAuth
getAccessToken(api_path,"switzerlandnorth")
translateR:::getAccessToken("30802f09feb943b0b90a2e781df06b69")
library(httr)
POST("https://api.cognitive.microsoft.com/sts/v1.0/issueToken?Subscription-Key=30802f09feb943b0b90a2e781df06b69", body = "")
connectApiUrl <- "https://api.cognitive.microsoft.com"
connectApiKey <- "30802f09feb943b0b90a2e781df06b69"
resp <- GET(connectApiUrl,
add_headers(Authorization = paste0("Key ", connectApiKey)))
result <- content(resp, "parsed")
print(result)
connectApiUrl <- "https://api.cognitive.microsoft.com/sts/v1.0/"
connectApiKey <- "30802f09feb943b0b90a2e781df06b69"
resp <- GET(connectApiUrl,
add_headers(Authorization = paste0("Key ", connectApiKey)))
result <- content(resp, "parsed")
print(result)
