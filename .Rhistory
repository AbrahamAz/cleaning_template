install.packages("translateR")
install.packages("pacman")
pacman::p_load(translateR)
library(translateR)
help(translateR)
??translateR
install.packages("translateR")
install.packages("translateR")
R.Version()
R.Version()
library(dplyr)
R.Version()
remove.packages("dbplyr")
remove.packages("dtplyr")
installed_packages <- rownames(installed.packages())
remove.packages(installed_packages)
install.packages("slam")
library(slam)
sentences <- df$sentences
preprocessed_sentences <- sapply(sentences, preprocess_text)
library(tm)
library(proxy)
library(text)
library(slam)
preprocess_text <- function(text) {
# You can add more preprocessing steps based on your specific requirements
text <- tolower(text)
text <- removePunctuation(text)
text <- removeNumbers(text)
text <- stripWhitespace(text)
return(text)
}
df <- data.frame(
id = as.character(1:10),
sentences = c(
"As the sun dipped below the horizon, the sky transformed into a canvas of warm hues, with streaks of orange, pink, and purple painting a breathtaking masterpiece.",
"The morning sky erupted in a kaleidoscope of colors, with the first light of dawn casting a golden glow that gradually merged into the cool shades of the waking sky.",
"A vivid sunset unfolded, casting a symphony of colors across the heavens, as if nature itself were putting on a spectacular show to bid farewell to the day.",
"Underneath a canopy of stars, the midnight sky shimmered with an array of celestial colors, creating a tranquil and mesmerizing scene.",
"Skyscrapers reached towards the heavens, their reflective surfaces capturing the vibrant energy of the city that never slept.",
"Amidst the urban hustle and bustle, neon lights adorned the streets, turning the city into a luminous mosaic of life and activity.",
"In the heart of the city, historic architecture stood as a testament to its rich past, juxtaposed against the modern skyline that embraced progress.",
"The rhythmic ebb and flow of the tide created a soothing melody, as the sea whispered secrets to the shoreline.",
"Endless waves kissed the sandy beach, leaving behind a trail of foamy lace as a gentle reminder of the sea's eternal dance.",
"A vast expanse of blue stretched to the horizon, where the sky met the sea in a seamless blend of azure, creating a sense of boundless tranquility."
)
)
sentences <- df$sentences
preprocessed_sentences <- sapply(sentences, preprocess_text)
dtm <- DocumentTermMatrix(Corpus(VectorSource(preprocessed_sentences)))
term_freq <- as.matrix(dtm)
matrix <- as.matrix(dfm)
dfm <- as.DocumentTermMatrix(term_freq)
model <- doc2vec(preprocessed_sentences)
setwd("D:/R-tools/cleaning_template")
knitr::opts_chunk$set(echo = TRUE)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
rm(list = ls())
directory_dictionary <- list(
research_cycle_name = 'xxxx',
round = 'xxxx',
dir.audits = "data/inputs/audits/reach/", # The directory to your audit files
dir.audits.check = "output/checking/audit/",# The directory to your audit summary files (you'll be checking these)
dir.requests = "output/checking/requests/", # the directory of your other_requests file
dir.responses = "output/checking/responses/", # the directory of your responses to open questions
enum_colname = "enum_id", # the column that contains the enumerator ID,
enum_comments = 'XXX', # the column that contains the enumerator's comments,
filename.tool = "resources/UKR2207_Questionnaire_CCCM_R12_MAR2023_v1.xlsx", # the name of your Kobo tool and its path
data_name = "XXXX.xlsx", # the name of your dataframe
data_path = "data/inputs/kobo_export/", # the path to your dataframe
label_colname = 'label::English', # the name of your label column. Has to be identical in Kobo survey and choices sheets
dctime_short = "XXXX" # the data of your survey (just for naming)
)
api_key <- source('resources/microsoft.api.key_ukraine_new.R')$value
#-------------------------------Initialize packages, load tools -----------------------------
source("src/init.R")
source("src/load_Data.R")
source('src/sections/process_old_data.R')
# final preparation
# Rename your dataframes
raw.main <- kobo.raw.main
sheet_names <- sheet_names[sheet_names!='kobo.raw.main']
sheet_names_new <- gsub('kobo.','',sheet_names)
if(length(sheet_names_new)>0){
for(i in 1:length(sheet_names_new)){
txt <- paste0(sheet_names_new[i],' <- ',sheet_names[i])
eval(parse(text=txt))
}
}
# select the columns in your data that contain date elements
date_cols_main <- c("start","end", tool.survey %>% filter(type == "date" & datasheet == "main") %>% pull(name),
"submission_time") # add them here
# transform them into the datetime format
raw.main <- raw.main %>%
mutate_at(date_cols_main, ~ifelse(!str_detect(., '-'), as.character(convertToDateTime(as.numeric(.))), .))
rm(date_cols_main)
# If there were any changes in the tool during data collection, they can be run here
source('src/sections/tool_modification.R')
min_duration_interview <- 5 # minimum duration of an interview (screen time in minutes)
max_duration_interview <- 60 # maximum duration of an interview (screen time in minutes)
pre_process_audit_files <- F # whether cases of respondent taking too long to answer 1 question should cleaned.
# if pre_process_audit_files =T, enter the maximum time that  the respondent can spend answering 1 question (in minutes)
max_length_answer_1_question <- 20
# Used during the check for soft duplicates.
# The minimum number of different columns that makes us confident that the entry is not a soft duplicate
min_num_diff_questions <- 8
# load your audit files
audits <- utilityR::load.audit.files(directory_dictionary$dir.audits, uuids = raw.main$uuid, track.changes = F)
directory_dictionary$dir.audits
directory_dictionary$dir.audits = "data/inputs/audits/"
# load your audit files
audits <- utilityR::load.audit.files(directory_dictionary$dir.audits, uuids = raw.main$uuid, track.changes = F)
# add 2 more columns to make readable start and end columns
audits$start_readable <- as.POSIXct(audits$start / 1000, origin = "1970-01-01")
audits$end_readable <- as.POSIXct(audits$end / 1000, origin = "1970-01-01")
View(audits)
View(audits)
unique(audits$event)
audits.summary <- audits %>%
filter(event !='group.questions') %>%
group_by(uuid) %>%
group_modify(~utilityR::process.uuid(.x))
View(audits.summary)
names(audits.summary)
audits.summary <- audits %>%
filter(event !='group.questions') %>%
group_by(uuid) %>%
group_modify(~utilityR::process.uuid(.x)) %>%
ungroup() %>%
mutate(tot.rt = tot.rt+tot.rt.inter)
view(head(audits.summary))
view(audits[audits$uuid=='00c4e502-2a96-498d-b506-9cd04b85cec8',])
view(audits[audits$event != 'group.questions' & audits$uuid=='00c4e502-2a96-498d-b506-9cd04b85cec8',])
View(audits)
view(audits[audits$event != 'group.questions' & audits$uuid=='00c4e502-2a96-498d-b506-9cd04b85cec8',])
view(audits[ audits$uuid=='00c4e502-2a96-498d-b506-9cd04b85cec8',])
audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T)
audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>% filter(uuid=='00c4e502-2a96-498d-b506-9cd04b85cec8') %>% View()
audits %>%
distinct(duration, uuid,start_readable,end_readable, .keep_all = T) %>% filter(uuid=='00c4e502-2a96-498d-b506-9cd04b85cec8') %>% View()
audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>% filter(inter_q_duration<0,uuid=='00c4e502-2a96-498d-b506-9cd04b85cec8') %>% View()
audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>% filter(!inter_q_duration<0,uuid=='00c4e502-2a96-498d-b506-9cd04b85cec8') %>% View()
audits.summary <- audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>%
filter(!inter_q_duration<0) %>%
group_by(uuid) %>%
group_modify(~utilityR::process.uuid(.x)) %>%
ungroup() %>%
mutate(tot.rt = tot.rt+tot.rt.inter)
audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>%
#filter(!inter_q_duration<0) %>%
group_by(uuid) %>%
group_modify(~utilityR::process.uuid(.x)) %>%
ungroup() %>%
mutate(tot.rt = tot.rt+tot.rt.inter)
audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>%
filter(!inter_q_duration<0 & event !='form.start') %>%
group_by(uuid) %>%
group_modify(~utilityR::process.uuid(.x)) %>%
ungroup() %>%
mutate(tot.rt = tot.rt+tot.rt.inter)
audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>%
filter(!inter_q_duration<0 & event !='form.start')
audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T)
audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>%
filter(!(inter_q_duration<0 & event !='form.start'))
audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>%
filter(!(inter_q_duration<0 & event !='form.start')) %>%
group_by(uuid) %>%
group_modify(~utilityR::process.uuid(.x)) %>%
ungroup() %>%
mutate(tot.rt = tot.rt+tot.rt.inter)
audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>%
filter(!(inter_q_duration<0 & (event !='form.start' | event!='form.exit')))
audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>%
filter(!(inter_q_duration<0 & (event !='form.start' | event!='form.exit'))) %>%
group_by(uuid) %>%
group_modify(~utilityR::process.uuid(.x)) %>%
ungroup() %>%
mutate(tot.rt = tot.rt+tot.rt.inter)
audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>%
filter(!(inter_q_duration<0 & !event %in% c('form.exit','form.start')))
audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>%
filter(!(inter_q_duration<0 & !event %in% c('form.exit','form.start'))) %>%
group_by(uuid) %>%
group_modify(~utilityR::process.uuid(.x)) %>%
ungroup() %>%
mutate(tot.rt = tot.rt+tot.rt.inter)
warnings()
View(audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>%
filter(!(inter_q_duration<0 & !event %in% c('form.exit','form.start'))))
View(audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>%
filter(!(inter_q_duration<0 & !event %in% c('form.exit','form.start'))))
audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>%
filter(!(inter_q_duration %_<_% 0 & !event %in% c('form.exit','form.start'))) %>%
group_by(uuid) %>%
group_modify(~utilityR::process.uuid(.x)) %>%
ungroup() %>%
mutate(tot.rt = tot.rt+tot.rt.inter)
View(audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>%
filter(!(inter_q_duration%_<_%0 & !event %in% c('form.exit','form.start'))))
audits.summary <- audits %>%
distinct(inter_q_duration,duration, uuid,start_readable,end_readable, .keep_all = T) %>%
filter(!(inter_q_duration %_<_% 0 & !event %in% c('form.exit','form.start'))) %>%
group_by(uuid) %>%
group_modify(~utilityR::process.uuid(.x)) %>%
ungroup() %>%
mutate(tot.rt = tot.rt+tot.rt.inter)
View(audits.summary)
# get the additional data from the main df
data.audit <- raw.main %>%
dplyr::mutate(duration_mins = difftime(end, start, units = 'mins'),
num_NA_cols = rowSums(is.na(raw.main)),
num_dk_cols = rowSums(select(., matches("dk_undec")), na.rm = T),
num_other_cols = rowSums(!is.na(raw.main[str_ends(colnames(raw.main), "_other")]), na.rm = T)) %>%
select(uuid, !!sym(directory_dictionary$enum_colname), start, end, duration_mins, num_NA_cols, num_dk_cols, num_other_cols)
# Generate the audits_summary file - General info about each interview
audits.summary <- data.audit %>%
left_join(audits.summary, by="uuid") %>% select(-contains("/")) %>%
relocate(uuid, duration_mins, num_NA_cols, num_dk_cols, num_other_cols, tot.rt) %>%
arrange(duration_mins)
View(audits.summary)
# follow up with FPs if there are surveys under 10 minutes or above 1 hour
survey_durations_check <- audits.summary %>% filter(tot.rt < min_duration_interview | tot.rt > max_duration_interview)
View(survey_durations_check)
knitr::opts_chunk$set(echo = TRUE)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
rm(list = ls())
directory_dictionary <- list(
research_cycle_name = 'xxxx',
round = 'xxxx',
dir.audits = "data/inputs/audits/", # The directory to your audit files
dir.audits.check = "output/checking/audit/",# The directory to your audit summary files (you'll be checking these)
dir.requests = "output/checking/requests/", # the directory of your other_requests file
dir.responses = "output/checking/responses/", # the directory of your responses to open questions
enum_colname = "XXX", # the column that contains the enumerator ID,
enum_comments = 'XXX', # the column that contains the enumerator's comments,
filename.tool = "resources/XXX.xlsx", # the name of your Kobo tool and its path
data_name = "XXXX.xlsx", # the name of your dataframe
data_path = "data/inputs/kobo_export/", # the path to your dataframe
label_colname = 'label::English', # the name of your label column. Has to be identical in Kobo survey and choices sheets
dctime_short = "XXXX" # the data of your survey (just for naming)
)
pacman::p_load(docstring, tidyverse, readxl, writexl, openxlsx, stringr,
sf, geosphere, qdapRegex, cluster, randomcoloR, svDialogs, scales, janitor, utilityR,zip,geosphere,
robotoolbox)
token <- "e5895090cb45140ad364f68b8592d32a38bf007a" # check your token at https://kobo.impact-initiatives.org/token/?format=json
kobo_setup(url = "https://kobo.impact-initiatives.org/",
token = token)
kobo_settings()
kobo_asset_list()
assest_uid <- 'aBB5WE2RVBfUaDRctcj7pC' # you can check all your available assets with kobo_asset_list() and select the correct uid out of that
asset <- kobo_asset(assest_uid)
audits <- kobo_audit(asset)
kobo_audit
show(kobo_audit)
methods(kobo_audit)
help(kobo_audit)
methods(class="kobo_asset")
getAnywhere(kobo_audit.kobo_asset)
kobo_audit2 <- function (x) {
asset_version_list <- kobo_asset_version_list(x$uid)
asset_version_list <- filter(asset_version_list, .data$deployed)
cond <- nrow(asset_version_list) > 0
if (cond) {
version <- unique(asset_version_list$uid)
form <- lapply(version, function(v) kobo_form(x, v))
form <- lapply(form, function(v) as.character(v))
form <- list_rbind(form)
}
else {
form <- kobo_form(x)
}
if (!any("audit" %in% form$name))
abort("`audit` not enabled in the current version of the survey",
call = NULL)
kobo_audit_(x$uid)
}
audits <- kobo_audit2(asset)
kobo_asset_version_list(kobo_asset$uid)
kobo_asset_version_list(asset$uid)
x <- asset
asset_version_list <- kobo_asset_version_list(x$uid)
asset_version_list <- filter(asset_version_list, .data$deployed)
asset_version_list
cond <- nrow(asset_version_list) > 0
cond
version <- unique(asset_version_list$uid)
form <- lapply(version, function(v) kobo_form(x, v))
View(form)
form <- lapply(form, function(v) as.character(v))
View(form)
form <- lapply(version, function(v) kobo_form(x, v))
form <- list_rbind(form)
!any("audit" %in% form$name)
kobo_audit_(x$uid)
robotoolbox:::kobo_audit_(x$uid)
uid <- x$uid
audit_meta <- get_audit_url_(uid)
audit_meta <- robotoolbox:::get_audit_url_(uid)
audit_meta
headers <- list(Authorization = paste("Token", Sys.getenv("KOBOTOOLBOX_TOKEN")))
reqs <- lapply(audit_meta$download_url, function(url) {
req <- HttpRequest$new(url, headers = headers)
req$retry("get", times = 3L, retry_only_on = c(500,
503), terminate_on = 404)
})
??HttpRequest
help(HttpRequest)
library(crul)
req <- HttpRequest$new(url, headers = headers)
req$retry("get", times = 3L, retry_only_on = c(500,
503), terminate_on = 404)
req
reqs <- lapply(audit_meta$download_url, function(url) {
req <- HttpRequest$new(url, headers = headers)
req$retry("get", times = 3L, retry_only_on = c(500,
503), terminate_on = 404)
})
reqs
sleep <- 0.01
res <- AsyncQueue$new(.list = reqs, bucket_size = Inf, sleep = sleep)
res$request()
cond <- any(res$status_code() >= 300L)
if (any(cond)) {
msg <- res$content()[cond]
abort(error_msg(msg[[1]]), call = NULL)
}
help(abort)
if (any(cond)) {
msg <- res$content()[cond]
rlang::abort(error_msg(msg[[1]]), call = NULL)
}
help(error_msg)
??error_msg
any(cond)
res$status_code()
res <- res$parse(encoding = "UTF-8")
res <- mutate(audit_meta, data = lapply(res, function(path) dt2tibble(fread(path))))
help(dt2tibble)
??dt2tibble
install.packages('taxize')
res <- mutate(audit_meta, data = lapply(res, function(path) robotoolbox::dt2tibble(fread(path))))
res <- mutate(audit_meta, data = lapply(res, function(path) robotoolbox:::dt2tibble(fread(path))))
res <- mutate(audit_meta, data = lapply(res, function(path) robotoolbox:::dt2tibble(data.table::fread(path))))
res <- select(res, -"download_url")
View(res)
View(res[[2]][[1]])
unnest(res, "data")
class(res)
class(res$data)
res$data
res$data[1]
lapply(res$data[1:2], as.character)
res$data[1]
res$data <- lapply(res$data, function(x) {
x[] <- lapply(x, as.numeric)
x
})
res$data <- lapply(res$data, function(x) {
x[] <- lapply(x, as.character)
x
})
res <- AsyncQueue$new(.list = reqs, bucket_size = Inf, sleep = sleep)
res$request()
cond <- any(res$status_code() >= 300L)
if (any(cond)) {
msg <- res$content()[cond]
rlang::abort(robotoolbox:::error_msg(msg[[1]]), call = NULL)
}
res <- res$parse(encoding = "UTF-8")
res <- mutate(audit_meta, data = lapply(res, function(path) robotoolbox:::dt2tibble(data.table::fread(path))))
res <- select(res, -"download_url")
res$data <- lapply(res$data, function(x) {
x[] <- lapply(x, as.character)
x
})
mutate(unnest(res, "data"), name = basename(.data$node),
.before = "start", start = as.POSIXct(.data$start/1000,
origin = "1970-01-01"), end = as.POSIXct(.data$end/1000,
origin = "1970-01-01"))
res <- AsyncQueue$new(.list = reqs, bucket_size = Inf, sleep = sleep)
res$request()
cond <- any(res$status_code() >= 300L)
if (any(cond)) {
msg <- res$content()[cond]
rlang::abort(robotoolbox:::error_msg(msg[[1]]), call = NULL)
}
res <- res$parse(encoding = "UTF-8")
res <- mutate(audit_meta, data = lapply(res, function(path) robotoolbox:::dt2tibble(data.table::fread(path))))
res <- select(res, -"download_url")
res$data <- lapply(res$data, function(x) {
x[['old-value']] <- as.character(x[['old-value']])
x[['new-value']] <- as.character(x[['new-value']])
x
})
mutate(unnest(res, "data"), name = basename(.data$node),
.before = "start", start = as.POSIXct(.data$start/1000,
origin = "1970-01-01"), end = as.POSIXct(.data$end/1000,
origin = "1970-01-01"))
audits <- mutate(unnest(res, "data"), name = basename(.data$node),
.before = "start", start = as.POSIXct(.data$start/1000,
origin = "1970-01-01"), end = as.POSIXct(.data$end/1000,
origin = "1970-01-01"))
audits$start <- as.numeric(strptime(audits$start,"%Y-%m-%d %H:%M:%S"))
audits$end <- as.numeric(strptime(audits$start,"%Y-%m-%d %H:%M:%S"))
View(audits)
View(audits)
detach("package:robotoolbox", unload = TRUE)
remove.packages("robotoolbox")
devtools::install_github('https://github.com/Nestor-Ch/robotoolbox')
devtools::install_github('https://github.com/Nestor-Ch/robotoolbox')
install.packages('dm')
devtools::install_github('https://github.com/Nestor-Ch/robotoolbox')
install.packages('dm',version='1.0.10.9000')
sessionInfo()
library(dm)
sessionInfo()
devtools::install_github('https://github.com/Nestor-Ch/robotoolbox')
detach("package:dm", unload = TRUE)
remove.packages("dm")
install.packages('dm',version='1.0.10.9000')
devtools::install_github("cynkra/dm")
devtools::install_github('https://github.com/Nestor-Ch/robotoolbox')
asset <- kobo_asset(assest_uid)
library(robotoolbox)
asset <- kobo_asset(assest_uid)
assest_uid
knitr::opts_chunk$set(echo = TRUE)
# use API key?
use_API <- T
knitr::opts_chunk$set(echo = TRUE)
token <- "e5895090cb45140ad364f68b8592d32a38bf007a" # check your token at https://kobo.impact-initiatives.org/token/?format=json
# connect to the server here
kobo_setup(url = "https://kobo.impact-initiatives.org/",
token = token)
assest_uid <- 'aBB5WE2RVBfUaDRctcj7pC' # you can check all your available assets with kobo_asset_list() and select the correct uid out of that
asset <- kobo_asset(assest_uid)
audits <- kobo_audit(asset)
View(audits)
dplyr::last_dplyr_warnings()
View(asset)
View(audits)
robotoolbox::kobo_auid_
robotoolbox:::kobo_auid_
robotoolbox:::kobo_auit_
robotoolbox:::kobo_audit_
View(res)
View(res[[2]][[1]])
uid
audit_meta <- get_audit_url_(uid)
audit_meta <- robotoolbox:::get_audit_url_(uid)
View(audit_meta)
View(audit_meta)
headers <- list(Authorization = paste("Token", Sys.getenv("KOBOTOOLBOX_TOKEN")))
reqs <- lapply(audit_meta$download_url, function(url) {
req <- HttpRequest$new(url, headers = headers)
req$retry("get", times = 3L, retry_only_on = c(500, 503),
terminate_on = 404)
})
help(HttpRequest)
reqs <- lapply(audit_meta$download_url, function(url) {
req <- crul::HttpRequest$new(url, headers = headers)
req$retry("get", times = 3L, retry_only_on = c(500, 503),
terminate_on = 404)
})
sleep <- 0.01
res <- AsyncQueue$new(.list = reqs, bucket_size = Inf, sleep = sleep)
library(crul)
res <- crul::AsyncQueue$new(.list = reqs, bucket_size = Inf, sleep = sleep)
res$request()
cond <- any(res$status_code() >= 300L)
res <- res$parse(encoding = "UTF-8")
col_classes <- c("event", "node", "old-value", "new-value",
"user")
col_classes <- setNames(rep("character", length(col_classes)),
col_classes)
res <- mutate(audit_meta, data = lapply(res, function(path) fread(path,
colClasses = col_classes, data.table = FALSE)))
res <- dplyr::mutate(audit_meta, data = lapply(res, function(path) fread(path,
colClasses = col_classes, data.table = FALSE)))
res <- dplyr::mutate(audit_meta, data = lapply(res, function(path) data.table::fread(path,
colClasses = col_classes, data.table = FALSE)))
View(res)
View(res[[3]][[1]])
