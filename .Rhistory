group_by(uuid) %>%
mutate(max_speed = max(speed, na.rm = T)) %>%
filter(any(max_speed>=top_allowed_speed)) %>%
select(-max_speed) %>%
ungroup() %>%
filter(speed>top_allowed_speed)
summary_speed_short <- summary_speed %>%
group_by(uuid) %>%
summarise(mean_speed = mean(speed, na.rm = T))
# fill up the excel file
wb <- createWorkbook()
addWorksheet(wb, "General table")
addWorksheet(wb, "Audit issues summary")
addWorksheet(wb, "Speed issues")
addWorksheet(wb, "Speed issues summary")
addWorksheet(wb, "Location issues")
addWorksheet(wb, "Location issues summary")
writeData(wb, "General table", general_table)
writeData(wb, "Audit issues summary", geo_processed_audits_issues)
writeData(wb, "Speed issues", summary_speed)
writeData(wb, "Speed issues summary", summary_speed_short)
# whether the points of the interview are in their correct squares
# general checks
if(!merge_column %in% colnames(raw.main)){
stop('The merge_column with polygon names is not present in your data')
}
sf_use_s2(TRUE)
admin_boundary <- st_read(dsn = polygon_file)
if(! polygon_file_merge_column %in% names(admin_boundary)){
stop('The polygon_file_merge_column with polygon names is not present in your json file')
}
# select only geometry and the ID of the polygon and make valid
admin_boundary_select <-  admin_boundary %>%
select(!!sym(polygon_file_merge_column)) %>%
st_make_valid()
# select only needed columns from the general_table
collected_pts <- general_table %>%
select(uuid, latitude,longitude,variable_explanation,question) %>%
left_join(raw.main %>% select(uuid,!!sym(merge_column)))
# set the crs and ensure they're the same
collected_sf <- collected_pts %>% st_as_sf(coords = c('longitude','latitude'), crs = "+proj=longlat +datum=WGS84")
admin_boundary_select <- st_transform(admin_boundary_select, crs = "+proj=longlat +datum=WGS84")
sf_use_s2(FALSE)
spatial_join <- st_join(collected_sf, admin_boundary_select, join = st_within) %>%
st_drop_geometry() %>%
mutate(GPS_MATCH = case_when(
is.na(!!sym(polygon_file_merge_column)) ~ "Outside polygon",
!!sym(polygon_file_merge_column) == !!sym(merge_column) ~ "Correct polygon",
.default = "Wrong polygon"
))
if(any(spatial_join$GPS_MATCH !="Correct polygon")){
check_spatial <- tibble(spatial_join) %>%
group_by(uuid) %>%
filter(any(GPS_MATCH != "Correct polygon")) %>%
ungroup()
summary_spatial <- check_spatial %>%
group_by(uuid) %>%
summarise(settlement = unique(settlement),
n_coordinates = n(),
wrong_coordinates = length(GPS_MATCH[GPS_MATCH!="Correct polygon"]))
writeData(wb, "Location issues", check_spatial)
writeData(wb, "Location issues summary", summary_spatial)
}else cat("All GPS points are matching their selected polygon :)")
saveWorkbook(wb, make.filename.xlsx("output/checking/audit/", "audit_checks_full"), overwrite = TRUE)
}
View(check_spatial)
check_spatial %>%
group_by(uuid) %>%
summarise(indicated_location = unique(settlement),
n_coordinates = n(),
wrong_coordinates = length(GPS_MATCH[GPS_MATCH!="Correct polygon"]),
actual_location = paste0(unique(settlement) ,collapse = ', ')
)
summary_spatial <- check_spatial %>%
group_by(uuid) %>%
summarise(indicated_location = unique(settlement),
actual_location  = paste0(unique(settlement) ,collapse = ', '),
n_coordinates = n(),
wrong_coordinates = length(GPS_MATCH[GPS_MATCH!="Correct polygon"])
)
View(summary_spatial)
file.exists(make.filename.xlsx(directory_dictionary$dir.audits.check, "geospatial_check"))
deletion.log.coord <- readxl::read_excel(make.filename.xlsx(directory_dictionary$dir.audits.check, "geospatial_check"))
deletion.log.coord
raw.main %>% select(uuid, directory_dictionary$enum_colname)
general_table <- geo_processed_audits %>%
filter(!is.na(latitude)) %>%
group_by(uuid) %>%
mutate(lagget_lat = lag(latitude),
lagged_long = lag(longitude)
) %>%
mutate(time_difference = ((start-dplyr::lag(end))/60000)/60,
distance = distHaversine(cbind(longitude,latitude), cbind(lagged_long,lagget_lat)),
time_difference = ifelse(round(time_difference,2)==0, NA,time_difference ),
distance = ifelse(distance<=(accuracy), NA,distance),
speed = round((distance/1000)/time_difference,2)
) %>%
select(-lagget_lat,-lagged_long) %>%
ungroup() %>%
mutate(start = as.POSIXct(start / 1000, origin = "1970-01-01"),
end = as.POSIXct(end / 1000, origin = "1970-01-01")) %>%
left_join(raw.main %>% select(uuid, directory_dictionary$enum_colname))
general_table
View(deletion.log.coord)
geo_processed_audits %>%
filter(!is.na(latitude)) %>%
group_by(uuid) %>%
mutate(lagget_lat = lag(latitude),
lagged_long = lag(longitude)
) %>%
mutate(time_difference = ((start-dplyr::lag(end))/60000)/60,
distance = distHaversine(cbind(longitude,latitude), cbind(lagged_long,lagget_lat)),
time_difference = ifelse(round(time_difference,2)==0, NA,time_difference ),
distance = ifelse(distance<=(accuracy), NA,distance),
speed = round((distance/1000)/time_difference,2)
) %>%
select(-lagget_lat,-lagged_long) %>%
ungroup() %>%
mutate(start = as.POSIXct(start / 1000, origin = "1970-01-01"),
end = as.POSIXct(end / 1000, origin = "1970-01-01")) %>%
left_join(raw.main %>% select(uuid, directory_dictionary$enum_colname)) %>%
rename(col_enum = directory_dictionary$enum_colname))
geo_processed_audits %>%
filter(!is.na(latitude)) %>%
group_by(uuid) %>%
mutate(lagget_lat = lag(latitude),
lagged_long = lag(longitude)
) %>%
mutate(time_difference = ((start-dplyr::lag(end))/60000)/60,
distance = distHaversine(cbind(longitude,latitude), cbind(lagged_long,lagget_lat)),
time_difference = ifelse(round(time_difference,2)==0, NA,time_difference ),
distance = ifelse(distance<=(accuracy), NA,distance),
speed = round((distance/1000)/time_difference,2)
) %>%
select(-lagget_lat,-lagged_long) %>%
ungroup() %>%
mutate(start = as.POSIXct(start / 1000, origin = "1970-01-01"),
end = as.POSIXct(end / 1000, origin = "1970-01-01")) %>%
left_join(raw.main %>% select(uuid, directory_dictionary$enum_colname)) %>%
rename(col_enum = directory_dictionary$enum_colname)
general_table <- geo_processed_audits %>%
filter(!is.na(latitude)) %>%
group_by(uuid) %>%
mutate(lagget_lat = lag(latitude),
lagged_long = lag(longitude)
) %>%
mutate(time_difference = ((start-dplyr::lag(end))/60000)/60,
distance = distHaversine(cbind(longitude,latitude), cbind(lagged_long,lagget_lat)),
time_difference = ifelse(round(time_difference,2)==0, NA,time_difference ),
distance = ifelse(distance<=(accuracy), NA,distance),
speed = round((distance/1000)/time_difference,2)
) %>%
select(-lagget_lat,-lagged_long) %>%
ungroup() %>%
mutate(start = as.POSIXct(start / 1000, origin = "1970-01-01"),
end = as.POSIXct(end / 1000, origin = "1970-01-01")) %>%
left_join(raw.main %>% select(uuid, directory_dictionary$enum_colname)) %>%
rename(col_enum = directory_dictionary$enum_colname)
general_table %>%
group_by(uuid) %>%
mutate(max_speed = max(speed, na.rm = T)) %>%
filter(any(max_speed>=top_allowed_speed)) %>%
select(-max_speed) %>%
ungroup() %>%
filter(speed>top_allowed_speed)
# get the table with interview speeds for interviewers who were moving too fast
summary_speed <- general_table %>%
group_by(uuid) %>%
mutate(max_speed = max(speed, na.rm = T)) %>%
filter(any(max_speed>=top_allowed_speed)) %>%
select(-max_speed) %>%
ungroup() %>%
filter(speed>top_allowed_speed)
general_table %>%
group_by(uuid) %>%
mutate(max_speed = max(speed, na.rm = T)) %>%
filter(any(max_speed>=top_allowed_speed)) %>%
select(-max_speed) %>%
ungroup() %>%
filter(speed>top_allowed_speed)
# get the general table with average speed per problematic interviewer
summary_speed_short <- summary_speed %>%
group_by(uuid) %>%
summarise(mean_speed = mean(speed, na.rm = T),
col_enum = unique(col_enum))
summary_speed_short
# select only needed columns from the general_table
collected_pts <- general_table %>%
select(uuid, latitude,longitude,variable_explanation,question,col_enum) %>%
left_join(raw.main %>% select(uuid,!!sym(merge_column)))
# set the crs and ensure they're the same
collected_sf <- collected_pts %>% st_as_sf(coords = c('longitude','latitude'), crs = "+proj=longlat +datum=WGS84")
admin_boundary_select <- st_transform(admin_boundary_select, crs = "+proj=longlat +datum=WGS84")
sf_use_s2(FALSE)
spatial_join <- st_join(collected_sf, admin_boundary_select, join = st_within) %>%
st_drop_geometry() %>%
mutate(GPS_MATCH = case_when(
is.na(!!sym(polygon_file_merge_column)) ~ "Outside polygon",
!!sym(polygon_file_merge_column) == !!sym(merge_column) ~ "Correct polygon",
.default = "Wrong polygon"
))
check_spatial
check_spatial <- tibble(spatial_join) %>%
group_by(uuid) %>%
filter(any(GPS_MATCH != "Correct polygon")) %>%
ungroup()
check_spatial
if(use_audit==T){
if( !exists('audits')){
audits <- utilityR::load.audit.files(directory_dictionary$dir.audits, uuids = raw.main$uuid, track.changes = F)
}
geo_processed_audits <- audits %>%
dplyr::group_by(uuid) %>%
dplyr::group_modify(~process.audit.geospatial2(.x, start_q ='informed_consent', end_q = 'j2_1_barriers_access_education')) %>%
dplyr::ungroup()
geo_processed_audits_issues <- geo_processed_audits %>%
filter(variable_explanation=='issue',
!grepl('is not present for this uuid',issue)) %>%
select(where(~!all(is.na(.x))), -c(question,variable_explanation))
general_table <- geo_processed_audits %>%
filter(!is.na(latitude)) %>%
group_by(uuid) %>%
mutate(lagget_lat = lag(latitude),
lagged_long = lag(longitude)
) %>%
mutate(time_difference = ((start-dplyr::lag(end))/60000)/60,
distance = distHaversine(cbind(longitude,latitude), cbind(lagged_long,lagget_lat)),
time_difference = ifelse(round(time_difference,2)==0, NA,time_difference ),
distance = ifelse(distance<=(accuracy), NA,distance),
speed = round((distance/1000)/time_difference,2)
) %>%
select(-lagget_lat,-lagged_long) %>%
ungroup() %>%
mutate(start = as.POSIXct(start / 1000, origin = "1970-01-01"),
end = as.POSIXct(end / 1000, origin = "1970-01-01")) %>%
left_join(raw.main %>% select(uuid, directory_dictionary$enum_colname)) %>%
rename(col_enum = directory_dictionary$enum_colname)
# get the table with interview speeds for interviewers who were moving too fast
summary_speed <- general_table %>%
group_by(uuid) %>%
mutate(max_speed = max(speed, na.rm = T)) %>%
filter(any(max_speed>=top_allowed_speed)) %>%
select(-max_speed) %>%
ungroup() %>%
filter(speed>top_allowed_speed)
# get the general table with average speed per problematic interviewer
summary_speed_short <- summary_speed %>%
group_by(uuid) %>%
summarise(mean_speed = mean(speed, na.rm = T),
col_enum = unique(col_enum))
# fill up the excel file
wb <- createWorkbook()
addWorksheet(wb, "General table")
addWorksheet(wb, "Audit issues summary")
addWorksheet(wb, "Speed issues")
addWorksheet(wb, "Speed issues summary")
addWorksheet(wb, "Location issues")
addWorksheet(wb, "Location issues summary")
writeData(wb, "General table", general_table)
writeData(wb, "Audit issues summary", geo_processed_audits_issues)
writeData(wb, "Speed issues", summary_speed)
writeData(wb, "Speed issues summary", summary_speed_short)
# whether the points of the interview are in their correct squares
# general checks
if(!merge_column %in% colnames(raw.main)){
stop('The merge_column with polygon names is not present in your data')
}
sf_use_s2(TRUE)
admin_boundary <- st_read(dsn = polygon_file)
if(! polygon_file_merge_column %in% names(admin_boundary)){
stop('The polygon_file_merge_column with polygon names is not present in your json file')
}
# select only geometry and the ID of the polygon and make valid
admin_boundary_select <-  admin_boundary %>%
select(!!sym(polygon_file_merge_column)) %>%
st_make_valid()
# select only needed columns from the general_table
collected_pts <- general_table %>%
select(uuid, latitude,longitude,variable_explanation,question,col_enum) %>%
left_join(raw.main %>% select(uuid,!!sym(merge_column)))
# set the crs and ensure they're the same
collected_sf <- collected_pts %>% st_as_sf(coords = c('longitude','latitude'), crs = "+proj=longlat +datum=WGS84")
admin_boundary_select <- st_transform(admin_boundary_select, crs = "+proj=longlat +datum=WGS84")
sf_use_s2(FALSE)
spatial_join <- st_join(collected_sf, admin_boundary_select, join = st_within) %>%
st_drop_geometry() %>%
mutate(GPS_MATCH = case_when(
is.na(!!sym(polygon_file_merge_column)) ~ "Outside polygon",
!!sym(polygon_file_merge_column) == !!sym(merge_column) ~ "Correct polygon",
.default = "Wrong polygon"
))
if(any(spatial_join$GPS_MATCH !="Correct polygon")){
check_spatial <- tibble(spatial_join) %>%
group_by(uuid) %>%
filter(any(GPS_MATCH != "Correct polygon")) %>%
ungroup()
summary_spatial <- check_spatial %>%
group_by(uuid) %>%
summarise(indicated_location = unique(settlement),
actual_location  = paste0(unique(settlement) ,collapse = ', '),
n_coordinates = n(),
wrong_coordinates = length(GPS_MATCH[GPS_MATCH!="Correct polygon"]),
col_enum = unique(col_enum)
) %>%
ungroup()
writeData(wb, "Location issues", check_spatial)
writeData(wb, "Location issues summary", summary_spatial)
}else cat("All GPS points are matching their selected polygon :)")
saveWorkbook(wb, make.filename.xlsx("output/checking/audit/", "audit_checks_full"), overwrite = TRUE)
}
if(use_audit==T){
if( !exists('audits')){
audits <- utilityR::load.audit.files(directory_dictionary$dir.audits, uuids = raw.main$uuid, track.changes = F)
}
geo_processed_audits <- audits %>%
dplyr::group_by(uuid) %>%
dplyr::group_modify(~process.audit.geospatial2(.x, start_q ='informed_consent', end_q = 'j2_1_barriers_access_education')) %>%
dplyr::ungroup()   %>%
left_join(raw.main %>% select(uuid, directory_dictionary$enum_colname)) %>%
rename(col_enum = directory_dictionary$enum_colname)
geo_processed_audits_issues <- geo_processed_audits %>%
filter(variable_explanation=='issue',
!grepl('is not present for this uuid',issue)) %>%
select(where(~!all(is.na(.x))), -c(question,variable_explanation))
general_table <- geo_processed_audits %>%
filter(!is.na(latitude)) %>%
group_by(uuid) %>%
mutate(lagget_lat = lag(latitude),
lagged_long = lag(longitude)
) %>%
mutate(time_difference = ((start-dplyr::lag(end))/60000)/60,
distance = distHaversine(cbind(longitude,latitude), cbind(lagged_long,lagget_lat)),
time_difference = ifelse(round(time_difference,2)==0, NA,time_difference ),
distance = ifelse(distance<=(accuracy), NA,distance),
speed = round((distance/1000)/time_difference,2)
) %>%
select(-lagget_lat,-lagged_long) %>%
ungroup() %>%
mutate(start = as.POSIXct(start / 1000, origin = "1970-01-01"),
end = as.POSIXct(end / 1000, origin = "1970-01-01"))
# get the table with interview speeds for interviewers who were moving too fast
summary_speed <- general_table %>%
group_by(uuid) %>%
mutate(max_speed = max(speed, na.rm = T)) %>%
filter(any(max_speed>=top_allowed_speed)) %>%
select(-max_speed) %>%
ungroup() %>%
filter(speed>top_allowed_speed)
# get the general table with average speed per problematic interviewer
summary_speed_short <- summary_speed %>%
group_by(uuid) %>%
summarise(mean_speed = mean(speed, na.rm = T),
col_enum = unique(col_enum))
# fill up the excel file
wb <- createWorkbook()
addWorksheet(wb, "General table")
addWorksheet(wb, "Audit issues summary")
addWorksheet(wb, "Speed issues")
addWorksheet(wb, "Speed issues summary")
addWorksheet(wb, "Location issues")
addWorksheet(wb, "Location issues summary")
writeData(wb, "General table", general_table)
writeData(wb, "Audit issues summary", geo_processed_audits_issues)
writeData(wb, "Speed issues", summary_speed)
writeData(wb, "Speed issues summary", summary_speed_short)
# whether the points of the interview are in their correct squares
# general checks
if(!merge_column %in% colnames(raw.main)){
stop('The merge_column with polygon names is not present in your data')
}
sf_use_s2(TRUE)
admin_boundary <- st_read(dsn = polygon_file)
if(! polygon_file_merge_column %in% names(admin_boundary)){
stop('The polygon_file_merge_column with polygon names is not present in your json file')
}
# select only geometry and the ID of the polygon and make valid
admin_boundary_select <-  admin_boundary %>%
select(!!sym(polygon_file_merge_column)) %>%
st_make_valid()
# select only needed columns from the general_table
collected_pts <- general_table %>%
select(uuid, latitude,longitude,variable_explanation,question,col_enum) %>%
left_join(raw.main %>% select(uuid,!!sym(merge_column)))
# set the crs and ensure they're the same
collected_sf <- collected_pts %>% st_as_sf(coords = c('longitude','latitude'), crs = "+proj=longlat +datum=WGS84")
admin_boundary_select <- st_transform(admin_boundary_select, crs = "+proj=longlat +datum=WGS84")
sf_use_s2(FALSE)
spatial_join <- st_join(collected_sf, admin_boundary_select, join = st_within) %>%
st_drop_geometry() %>%
mutate(GPS_MATCH = case_when(
is.na(!!sym(polygon_file_merge_column)) ~ "Outside polygon",
!!sym(polygon_file_merge_column) == !!sym(merge_column) ~ "Correct polygon",
.default = "Wrong polygon"
))
if(any(spatial_join$GPS_MATCH !="Correct polygon")){
check_spatial <- tibble(spatial_join) %>%
group_by(uuid) %>%
filter(any(GPS_MATCH != "Correct polygon")) %>%
ungroup()
summary_spatial <- check_spatial %>%
group_by(uuid) %>%
summarise(indicated_location = unique(settlement),
actual_location  = paste0(unique(settlement) ,collapse = ', '),
n_coordinates = n(),
wrong_coordinates = length(GPS_MATCH[GPS_MATCH!="Correct polygon"]),
col_enum = unique(col_enum)
) %>%
ungroup()
writeData(wb, "Location issues", check_spatial)
writeData(wb, "Location issues summary", summary_spatial)
}else cat("All GPS points are matching their selected polygon :)")
saveWorkbook(wb, make.filename.xlsx("output/checking/audit/", "audit_checks_full"), overwrite = TRUE)
}
file.exists( make.filename.xlsx("output/checking/audit/", "audit_checks_full"))
directory_dictionary$dir.audits.check
make.filename.xlsx(directory_dictionary$dir.audits.check, "audit_checks_full")
deletion.log.audit.issues <- readxl::read_excel( make.filename.xlsx(directory_dictionary$dir.audits.check,
"audit_checks_full"), sheet = 'Audit issues summary')
deletion.log.audit.issues
deletion.log.audit.issues <- readxl::read_excel( make.filename.xlsx(directory_dictionary$dir.audits.check,
"audit_checks_full"), sheet = 'Audit issues summary') %>%
rename(reason = issue)
deletion.log.audit.issues
readxl::read_excel( make.filename.xlsx(directory_dictionary$dir.audits.check,
"audit_checks_full"), sheet = 'Speed issues summary')
readxl::read_excel( make.filename.xlsx(directory_dictionary$dir.audits.check,
"audit_checks_full"), sheet = 'Speed issues summary') %>%
mutate(reason = 'Enumerator\'s movement speed throught the interview is not realistic') %>%
select(-mean_speed)
check_spatial
check_spatial %>%
group_by(uuid) %>%
summarise(indicated_location = unique(!!sym(merge_column)),
actual_location  = paste0(unique(!!sym(polygon_file_merge_column)) ,collapse = ', '),
n_coordinates = n(),
wrong_coordinates = length(GPS_MATCH[GPS_MATCH!="Correct polygon"]),
col_enum = unique(col_enum)
) %>%
ungroup()
summary_spatial <- check_spatial %>%
group_by(uuid) %>%
summarise(indicated_location = unique(!!sym(merge_column)),
actual_location  = paste0(unique(!!sym(polygon_file_merge_column)) ,collapse = ', '),
n_coordinates = n(),
wrong_coordinates = length(GPS_MATCH[GPS_MATCH!="Correct polygon"]),
col_enum = unique(col_enum)
) %>%
ungroup()
View(summary_spatial)
deletion.log.location <- readxl::read_excel( make.filename.xlsx(directory_dictionary$dir.audits.check,
"audit_checks_full"), sheet = 'Location issues summary')
deletion.log.location
deletion.log.location <- readxl::read_excel( make.filename.xlsx(directory_dictionary$dir.audits.check,
"audit_checks_full"), sheet = 'Location issues summary')
deletion.log.location
deletion.log.location <- deletion.log.location %>%
mutate(reason = 'Enumerator wasn\'t in the location indicated in the data') %>%
select(uuid, reason, col_enum)
deletion.log.location
deletion.log.audit.issues <- readxl::read_excel( make.filename.xlsx(directory_dictionary$dir.audits.check,
"audit_checks_full"), sheet = 'Audit issues summary') %>%
rename(reason = issue)
deletion.log.speed <- readxl::read_excel( make.filename.xlsx(directory_dictionary$dir.audits.check,
"audit_checks_full"), sheet = 'Speed issues summary')
if(nrow(deletion.log.speed)>0){
deletion.log.speed <- deletion.log.speed %>%
mutate(reason = 'Enumerator\'s movement speed throught the interview is not realistic') %>%
select(-mean_speed)
}else{
deletion.log.speed <- data.frame()
}
deletion.log.location <- readxl::read_excel( make.filename.xlsx(directory_dictionary$dir.audits.check,
"audit_checks_full"), sheet = 'Location issues summary')
if(nrow(deletion.log.location)>0){
deletion.log.location <- deletion.log.location %>%
mutate(reason = 'Enumerator wasn\'t in the location indicated in the data') %>%
select(uuid, reason, col_enum)
}else{
deletion.log.location <- data.frame()
}
deletion.log.audits <- rbind(deletion.log.location,deletion.log.audit.issues,deletion.log.speed)
deletion.log.audits %>% distinct(uuid)
deletion.log.new
View(summary_speed)
# get the table with interview speeds for interviewers who were moving too fast
summary_speed <- general_table %>%
group_by(uuid) %>%
mutate(max_speed = max(speed, na.rm = T)) %>%
filter(any(max_speed>=top_allowed_speed)) %>%
select(-max_speed) %>%
ungroup()
View(summary_speed)
View(check_spatial)
check_spatial <- tibble(spatial_join) %>%
group_by(uuid) %>%
filter(any(GPS_MATCH != "Correct polygon")) %>%
ungroup()
View(check_spatial)
View(general_table)
View(summary_speed_short)
View(summary_speed)
raw.main$oblat %>% unique()
raw.main$oblast %>% unique()
omit_locations <- T
location_column <- 'oblast' # doesn't have to match the merge_column if you're omitting other (larger geo levels)
location_ids <- c("UA26","UA21","UA80") # have to be present in the location_column!
ids_to_omit <- raw.main %>% filter(!!sym(location_column)%in% location_ids) %>% pull(uuid)
ids_to_omit
if(omit_locations){
ids_to_omit <- raw.main %>% filter(!!sym(location_column)%in% location_ids) %>% pull(uuid)
audits <- audits %>% filter(!uuid %in%ids_to_omit )
}
