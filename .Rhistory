) %>%
ungroup %>%
mutate(
ipq = issues/questions,
problematic_interview = ipq>0.3)
enum_issues <- base_issues %>%
group_by(!!sym(enum_name),!!sym(geo_column)) %>%
summarise(respondents = sum(respondents),
questions = sum(questions),
issues = sum(issues),
ipq = mean(ipq),
problematic_interview = sum(problematic_interview)) %>%
ungroup() %>%
mutate(ppr = problematic_interview/respondents)
#enum_issues$macroregion <- macro
enum_issues$list_used <- substr(ls[1],start = 1,stop = 4)
return(enum_issues)
}
}
anomaly_locator <- function(ls, audit,data,macro, enum_name){
general <- audit %>%
filter(question %in% ls,
#macroregion %in% macro,
duration >0.5) %>%
left_join(data %>%select(`uuid`, all_of(enum_name)) %>% distinct()) %>%
group_by(!!sym(enum_name)) %>%
mutate(n_resp = length(unique(uuid))) %>%
filter(
n_resp>10,
!is.na(!!sym(enum_name)),
!is.na(question),
!question %in% '') %>%
select(uuid,!!sym(enum_name),question, duration) %>%
mutate(duration = duration/60,
duration = ifelse(duration == 0, 00000000.1,duration)) %>%
mutate(duration = log(duration))
collected <- general %>%
left_join(general %>%
group_by(question) %>%
summarise(mean_q = mean(duration, na.rm = T),
sd_q = sd(duration, na.rm = T))) %>%
mutate(issue = abs(duration)<=(abs(mean_q)-2*abs(sd_q)))
base_issues <- collected %>%
group_by(!!sym(enum_name),uuid) %>%
summarise(
questions = length(question),
respondents = length(unique(uuid)),
issues = sum(issue, na.rm =T)
) %>%
ungroup %>%
mutate(
ipq = issues/questions,
problematic_interview = ipq>0.3)
enum_issues <- base_issues %>%
group_by(!!sym(enum_name)) %>%
summarise(respondents = sum(respondents),
questions = sum(questions),
issues = sum(issues),
ipq = mean(ipq),
problematic_interview = sum(problematic_interview)) %>%
ungroup() %>%
mutate(ppr = problematic_interview/respondents)
#enum_issues$macroregion <- macro
enum_issues$list_used <- substr(ls[1],start = 1,stop = 4)
return(enum_issues)
}
library(rstatix)
test_enum_differences <- function(variable_list, audits, data_frame, enum_name, oblast_col){
general <- audits %>%
filter(question %in% variable_list) %>%
left_join(data_frame %>%select(`uuid`, all_of(enum_name)) %>% distinct()) %>%
group_by(!!sym(enum_name)) %>%
mutate(n_resp = length(unique(uuid))) %>%
filter(
# n_resp>10,
!is.na(!!sym(enum_name)),
!is.na(question),
!question %in% '') %>%
select(uuid,!!sym(enum_name),question, duration) %>%
mutate(duration = duration/60,
duration = ifelse(duration == 0, 00000000.1,duration)) %>%
mutate(duration = log(duration))
general <- general %>%
left_join(data_frame %>%
select(uuid,all_of(oblast_col)) %>%
distinct())
txt <- paste0('duration ~ ', enum_name)
post_hoc_test_result <-  general %>%
group_by(!!sym(enum_name), uuid,!!sym(oblast_col)) %>%
summarise(duration = mean(duration)) %>%
ungroup() %>%
group_by(!!sym(oblast_col)) %>%
tukey_hsd(formula =  eval(parse(text = txt)))
result <- post_hoc_test_result %>%
filter(p.adj<=0.05)
result_fin <- rbind(result %>%
group_by(!!sym(oblast_col), group1) %>%
summarise(cnt = n()),
result %>%
group_by(!!sym(oblast_col), group2) %>%
summarise(cnt = n()) %>%
rename(group1 = group2)) %>%
group_by(!!sym(oblast_col), group1) %>%
summarise(cnt = sum(cnt)) %>%
left_join(raw.main %>%
select(!!sym(enum_name), !!sym(oblast_col)) %>%
distinct() %>%
group_by(!!sym(oblast_col)) %>%
summarise(cnt_en = n())) %>%
ungroup()
names(result_fin)[names(result_fin) %in% 'group1'] <- enum_name
result_fin <- result_fin %>%
mutate(perc_issues = cnt/cnt_en) %>%
left_join(
general %>%
group_by(!!sym(enum_name), !!sym(oblast_col)) %>%
summarise(duration = mean(duration)) %>%
left_join(general %>%
group_by(!!sym(oblast_col)) %>%
summarise(duration_ob = mean(duration)))
) %>%
filter(duration< duration_ob,
perc_issues > 0.5) %>%
rename(differences_from_enums = cnt,
total_enums = cnt_en) %>%
select(-duration, -duration_ob)
result_fin$list_used <- substr(variable_list[1],start = 1,stop = 4)
return(result_fin)
}
for (lst in ls_group){
#for (reg in unique(na.omit(audits$macroregion))){
res <- anomaly_locator(ls = lst,audit = audits,data = raw.main,
geo_column = 'reg',enum_name =directory_dictionary$enum_colname)
issues <- rbind(res, issues)
#}
}
anomaly_locator(ls = lst,audit = audits,data = raw.main,
geo_column = 'reg',enum_name =directory_dictionary$enum_colname)
anomaly_locator(ls = lst,audit = audits,data = raw.main,
geo_column = 'a1_3_oblast',enum_name ='a1_1_enumerator_id')
anomaly_locator <- function(ls, audit,data,geo_column, enum_name){
if(!all(ls %in% audit$question)){
nm <- setdiff(c(ls,geo_column,enum_name), audit$question)
stop(paste0("Error: some of the questions you've entered are not present in your data. please double check: ",
paste0(nm,collapse=', ') ))
}
general <- audit %>%
filter(question %in% ls,
duration >0.5) %>%
left_join(data %>%select(`uuid`, all_of(c(enum_name,geo_column))) %>% distinct()) %>%
group_by(!!sym(enum_name),!!sym(geo_column)) %>%
mutate(n_resp = length(unique(uuid))) %>%
filter(
n_resp>10,
!is.na(!!sym(enum_name)),
!is.na(question),
!question %in% '') %>%
select(uuid,!!sym(enum_name),!!sym(geo_column),question, duration) %>%
mutate(duration = duration/60,
duration = ifelse(duration == 0, 00000000.1,duration)) %>%
mutate(duration = log(duration))
if(nrow(general)>0){
collected <- general %>%
left_join(general %>%
group_by(question,!!sym(geo_column)) %>%
summarise(mean_q = mean(duration, na.rm = T),
sd_q = sd(duration, na.rm = T))) %>%
mutate(issue = abs(duration)<=(abs(mean_q)-2*abs(sd_q)))
base_issues <- collected %>%
group_by(!!sym(enum_name),uuid,!!sym(geo_column)) %>%
summarise(
questions = length(question),
respondents = length(unique(uuid)),
issues = sum(issue, na.rm =T)
) %>%
ungroup %>%
mutate(
ipq = issues/questions,
problematic_interview = ipq>0.3)
enum_issues <- base_issues %>%
group_by(!!sym(enum_name),!!sym(geo_column)) %>%
summarise(respondents = sum(respondents),
questions = sum(questions),
issues = sum(issues),
ipq = mean(ipq),
problematic_interview = sum(problematic_interview)) %>%
ungroup() %>%
mutate(ppr = problematic_interview/respondents)
#enum_issues$macroregion <- macro
enum_issues$list_used <- substr(ls[1],start = 1,stop = 4)
return(enum_issues)
}
}
anomaly_locator
for (lst in ls_group){
#for (reg in unique(na.omit(audits$macroregion))){
res <- anomaly_locator(ls = lst,audit = audits,data = raw.main,
geo_column = 'reg',enum_name =directory_dictionary$enum_colname)
issues <- rbind(res, issues)
#}
}
res <- anomaly_locator(ls = lst,audit = audits,data = raw.main,
geo_column = 'a1_3_oblast',enum_name ='a1_1_enumerator_id')
rlang::last_trace()
anomaly_locator <- function(ls, audit,data,geo_column, enum_name){
if(!all(c(ls,geo_column,enum_name) %in% audit$question)){
nm <- setdiff(c(ls,geo_column,enum_name), audit$question)
stop(paste0("Error: some of the questions you've entered are not present in your data. please double check: ",
paste0(nm,collapse=', ') ))
}
general <- audit %>%
filter(question %in% ls,
duration >0.5) %>%
left_join(data %>%select(`uuid`, all_of(c(enum_name,geo_column))) %>% distinct()) %>%
group_by(!!sym(enum_name),!!sym(geo_column)) %>%
mutate(n_resp = length(unique(uuid))) %>%
filter(
n_resp>10,
!is.na(!!sym(enum_name)),
!is.na(question),
!question %in% '') %>%
select(uuid,!!sym(enum_name),!!sym(geo_column),question, duration) %>%
mutate(duration = duration/60,
duration = ifelse(duration == 0, 00000000.1,duration)) %>%
mutate(duration = log(duration))
if(nrow(general)>0){
collected <- general %>%
left_join(general %>%
group_by(question,!!sym(geo_column)) %>%
summarise(mean_q = mean(duration, na.rm = T),
sd_q = sd(duration, na.rm = T))) %>%
mutate(issue = abs(duration)<=(abs(mean_q)-2*abs(sd_q)))
base_issues <- collected %>%
group_by(!!sym(enum_name),uuid,!!sym(geo_column)) %>%
summarise(
questions = length(question),
respondents = length(unique(uuid)),
issues = sum(issue, na.rm =T)
) %>%
ungroup %>%
mutate(
ipq = issues/questions,
problematic_interview = ipq>0.3)
enum_issues <- base_issues %>%
group_by(!!sym(enum_name),!!sym(geo_column)) %>%
summarise(respondents = sum(respondents),
questions = sum(questions),
issues = sum(issues),
ipq = mean(ipq),
problematic_interview = sum(problematic_interview)) %>%
ungroup() %>%
mutate(ppr = problematic_interview/respondents)
#enum_issues$macroregion <- macro
enum_issues$list_used <- substr(ls[1],start = 1,stop = 4)
return(enum_issues)
}
}
issues <- data.frame()
for (lst in ls_group){
#for (reg in unique(na.omit(audits$macroregion))){
res <- anomaly_locator(ls = lst,audit = audits,data = raw.main,
geo_column = 'reg',enum_name =directory_dictionary$enum_colname)
issues <- rbind(res, issues)
#}
}
setwd('C:/Users/reach/Desktop/old things/Sampling_MSNA2023/Sampling_MSNA2023')
library(sf)
library(tidyr)
library(openxlsx)
library(data.table)
library(magrittr)
library(here)
library(raster)
library(rgdal)
library(exactextractr)
library(rmapshaper)
library(openxlsx)
library(dplyr)
gc()
rpath <- "land_scan/tif/ukraine_hd.tif" # use this population raster for 2016 or newer of same resolution
pop_raster <- raster(rpath)
ADM4 <- st_read(dsn = "UKR_ADM4_2020.geojson") # ALWAYS use this file with settlement boundaries for ALL Ukraine
rpath <- "Consolidated sampling frame (draft).xlsx" # set path to your sampling frame (SF) file
SF <- read.xlsx(rpath) # choose a single sheet with your SF
SF <- SF %>% dplyr::select(-c("admin4Name_en","admin3Name_en","admin2Name_en","admin1Name_en","admin3Pcode","admin2Pcode" ,
"admin1Pcode"))
SF <- merge(SF, ADM4, by.x = "admin4Pcode", by.y = "admin4Pcode", all.x = TRUE, all.y = FALSE)
area_limit <- 10 # if the area of the polygon is 10 times smaller than what it's supposed to be, we drop it.
size_calibration_step <- 50 # by how many meters should we reduce the cell, if we can't find enough polygons within the admin unit?
use_buffer <- F
one_survey_polygon <- T
# ------------------------------Initial split -------------------------------
# set the coordinate systems of stuff
SF_utm <- st_as_sf(SF)%>%
st_transform(32236)
gc(full=T)
letterwrap <- function(n, depth = 1) {
args <- lapply(1:depth, FUN = function(x) return(LETTERS))
x <- do.call(expand.grid, args = list(args, stringsAsFactors = F))
x <- x[, rev(names(x)), drop = F]
x <- do.call(paste0, x)
if (n <= length(x)) return(x[1:n])
return(c(x, letterwrap(n - length(x), depth = depth + 1)))
}
SF_utm$letters
letterwrap(nrow(SF_utm))
SF_utm$letters <- letterwrap(nrow(SF_utm))
# get unique p codes
ls <- unique(SF_utm$admin4Pcode)
final_grid <-  NULL
rowid
gc()
rpath <- "Consolidated sampling frame (draft).xlsx" # set path to your sampling frame (SF) file
SF <- read.xlsx(rpath) # choose a single sheet with your SF
SF
head(SF)
SF <- SF %>% dplyr::select(-c("admin4Name_en","admin3Name_en","admin2Name_en","admin1Name_en","admin3Pcode","admin2Pcode" ,
"admin1Pcode"))
head(SF)
ADM4
class(SF)
class(ADM4)
SF <- merge(SF, ADM4, by.x = "admin4Pcode", by.y = "admin4Pcode", all.x = TRUE, all.y = FALSE)
class(SF)
SF
SF_utm <- st_as_sf(SF)%>%
st_transform(32236)
SF_utm
# set the raster crs
crs(pop_raster) <- crs(SF_utm)
mapview::mapview(ST_utm)
mapview::mapview(SF_utm)
mapview::mapview(pop_raster)
plot(pop_raster)
gc(full=T)
letterwrap <- function(n, depth = 1) {
args <- lapply(1:depth, FUN = function(x) return(LETTERS))
x <- do.call(expand.grid, args = list(args, stringsAsFactors = F))
x <- x[, rev(names(x)), drop = F]
x <- do.call(paste0, x)
if (n <= length(x)) return(x[1:n])
return(c(x, letterwrap(n - length(x), depth = depth + 1)))
}
SF_utm$letters <- letterwrap(nrow(SF_utm))
head(SF_utm$letters)
tail(SF_utm$letters)
# get unique p codes
ls <- unique(SF_utm$admin4Pcode)
ls
final_grid <-  NULL
i=1
ls[i]
View(SF)
rerun <- T # recursive loop parameter
# limit the frame to be only 1 settlemnt
SF_utm_area_poly <- SF_utm[SF_utm$admin4Pcode==ls[i],]
mapview::mapview(SF_utm_area_poly)
SF_utm_area_poly$Survey
# set the buffer size
if(SF_utm_area_poly$Survey>= 100){
buffer_perc <- 0.1
} else if (SF_utm_area_poly$Survey<100 & SF_utm_area_poly$Survey>=20){
buffer_perc <- 0.2
} else if (SF_utm_area_poly$Survey<20 & SF_utm_area_poly$Survey>=10){
buffer_perc <- 0.3
} else if (SF_utm_area_poly$Survey<10){
buffer_perc <- 0.5
}
buffer_perc
round(SF_utm_area_poly$Survey*buffer_perc,0)
# calculate the survey number with the buffer
SF_utm_area_poly$survey_buffer <- round(SF_utm_area_poly$Survey*buffer_perc,0)
SF_utm_area_poly$survey_total <- SF_utm_area_poly$Survey+SF_utm_area_poly$survey_buffer
SF_utm_area_poly$survey_total
# get the cell size depending on the settlement's population
pop <- SF_utm_area_poly$pop_upd
pop
if(pop >=50000){
cell_f <- 300
}else if(pop>=5000 & pop<50000){
cell_f <- 400
}else if(pop<5000){
cell_f <- 500
}
cell_f
"sfc_GEOMETRYCOLLECTION" %in% class(SF_utm_area_poly$geometry)
SF_utm_area_poly$pop_sum_all <- exact_extract(pop_raster, SF_utm_area_poly, 'sum')
SF_utm_area_poly$area <- st_area(SF_utm_area_poly)
SF_utm_area_poly$pop_sum_all
pop_raster
SF_utm_area_poly
SF_utm_area_poly$area <- st_area(SF_utm_area_poly)
SF_utm_area_poly$area
cell_f
# split the area into polygons
grid <- st_make_grid(st_as_sfc(st_bbox(SF_utm_area_poly)), cellsize = cell_f) %>%
st_intersection(SF_utm_area_poly)
mapview::mapview(grid)
plot(grid)
# check if enough polygons are present to cover the survey_total
cell_n <- cell_f
length(grid)
SF_utm_area_poly$survey_total
length(grid)<SF_utm_area_poly$survey_total
any(st_is(grid,'GEOMETRYCOLLECTION'))
grid <- st_as_sf(grid)
mapview::mapview(grid)
grid_pnt <- st_point_on_surface(grid)
grid_pnt
grid_SF <- st_join(grid_pnt, SF_utm_area_poly, left = TRUE)
grid_SF <- st_join(grid, grid_SF, left = TRUE)
grid_SF$area_rect <- st_area(grid_SF)
grid_SF$pop_sum_rect <- exact_extract(pop_raster, grid_SF, 'sum')
grid_SF$pop_sum_rect
crs(SF_utm)
crs(pop_raster)
# set the raster crs
crs(pop_raster) <- crs(SF_utm)
SF_utm <- st_as_sf(SF)%>%
st_transform(32236)
# set the raster crs
crs(pop_raster) <- crs(SF_utm)
grid_SF$pop_sum_rect <- exact_extract(pop_raster, grid_SF, 'sum')
grid_SF$pop_sum_rect
nrow(grid_SF[grid_SF$area_rect > units::set_units((cell_n^2)/area_limit,m^2) &
grid_SF$pop_sum_rect>0,])>=SF_utm_area_poly$survey_total
grid_SF[grid_SF$area_rect > units::set_units((cell_n^2)/area_limit,m^2)
grid_SF[grid_SF$area_rect > units::set_units((cell_n^2)/area_limit,m^2) &
grid_SF$pop_sum_rect>0,])
grid_SF[grid_SF$area_rect > units::set_units((cell_n^2)/area_limit,m^2) &
grid_SF$pop_sum_rect>0,]
pop_sum_rect
grid_SF$pop_sum_rect <- exact_extract(pop_raster, grid_SF, 'sum')
grid_SF$pop_sum_rect
pop_raster <- raster(rpath)
rpath <- "land_scan/tif/ukraine_hd.tif" # use this population raster for 2016 or newer of same resolution
pop_raster <- raster(rpath)
# set the raster crs
crs(pop_raster) <- crs(SF_utm)
exact_extract(pop_raster, grid_SF, 'sum')
sample(1:1000, nrow(grid_SF))
grid_SF$pop_sum_rect <- sample(1:1000, nrow(grid_SF))
grid_SF$pop_sum_rect
nrow(grid_SF[grid_SF$area_rect > units::set_units((cell_n^2)/area_limit,m^2) &
grid_SF$pop_sum_rect>0,])>=SF_utm_area_poly$survey_total
grid_SF <- grid_SF[grid_SF$area_rect > units::set_units((cell_n^2)/area_limit,m^2),]
mapview::mapview(grid_SF)
nrow(grid_SF[grid_SF$pop_sum_rect>0,])<SF_utm_area_poly$survey_total
grid_SF$pop_sum_all
grid_SF$pop_sum_rect
grid_SF$pop_prob <- sample(0:1,nrow(grid_SF))
grid_SF$pop_sum_rect
# get the probability of the polygon being sampled
grid_SF$pop_prob <- grid_SF$pop_sum_rect / 10000
grid_SF$pop_prob
grid_SF_sample <- grid_SF[,c("admin4Pcode","admin4Name_en","pop_prob","rect_id","survey_total")] # this code chunk takes values from raster population and assigns their 'weights' to your rectangles
grid_SF$num <- rowid(grid_SF$admin4Pcode)
grid_SF$rect_id <- paste(grid_SF$letters, grid_SF$num, sep = "_")
grid_SF_sample <- grid_SF[,c("admin4Pcode","admin4Name_en","pop_prob","rect_id","survey_total")] # this code chunk takes values from raster population and assigns their 'weights' to your rectangles
SF_utm_sample <- SF_utm_area_poly[,c("admin4Pcode","survey_total",'survey_buffer')] # ALWAYS check how your column with interviews is named, here it is 'Survey'
SF_utm_sample <- subset(SF_utm_sample, !(is.na(SF_utm_sample$survey_total) | SF_utm_sample$survey_total == 0))
SF_utm_sample
# drop polygons that are empty within the raster layer
grid_SF_sample <- subset(grid_SF_sample, !grid_SF_sample$pop_prob == 0)
grid_SF_sample <- subset(grid_SF_sample, grid_SF_sample$survey_total > 0)
grid_SF_sample <- grid_SF_sample[,c(1,3,4)]
names(grid_SF_sample)
SF_utm_sample <- SF_utm_sample %>% st_drop_geometry() %>% arrange(desc(survey_total))
# select survey_total number of random polygons from rect_id where
# relative population density is the probability distribution of being selected
samp_final <- sample(as.character(grid_SF_sample$rect_id),
SF_utm_sample$survey_total, replace=FALSE,
prob=grid_SF_sample$pop_prob)
samp_final
grid_SF <- grid_SF[grid_SF$rect_id %in% samp_final,]
mapview::mapview(grid_SF)
# specify polygons as optional and non-optional
# (buffer*2 of polygons with lower population density are considered optional)
grid_SF$class <- 'non-optional'
SF_utm_sample$survey_buffer>0
grid_SF <- grid_SF %>% arrange(pop_sum_rect)
survey_buffer
SF_utm_sample$survey_buffer
if(SF_utm_sample$survey_buffer>0){
grid_SF <- grid_SF %>% arrange(pop_sum_rect)
grid_SF[
1:(SF_utm_sample$survey_buffer*2),]$class <- 'interchangeable'
}
grid_SF$cell_size <- cell_n
final_grid <- rbind(grid_SF,final_grid)
print(paste0(round(i/length(ls),3)*100,'% done'))
Rect_Grid <- final_grid[,c("rect_id","admin4Pcode","admin4Name_en_1","admin2Name_en_1","admin1Name_en_1","Survey","admin4Name_ua",'survey_total','class')]
final_grid
names(c("rect_id","admin4Pcode","admin4Name_en_1","admin2Name_en_1","admin1Name_en_1","Survey","admin4Name_ua",'survey_total','class'),names(final_grid))
setdiff(names(c("rect_id","admin4Pcode","admin4Name_en_1","admin2Name_en_1","admin1Name_en_1","Survey","admin4Name_ua",'survey_total','class'),names(final_grid)))
setdiff(c("rect_id","admin4Pcode","admin4Name_en_1","admin2Name_en_1","admin1Name_en_1","Survey","admin4Name_ua",'survey_total','class'),names(final_grid)))
setdiff(c("rect_id","admin4Pcode","admin4Name_en_1","admin2Name_en_1","admin1Name_en_1","Survey","admin4Name_ua",'survey_total','class'),names(final_grid))
Rect_Grid <- final_grid[,c("rect_id","admin4Pcode","Survey","admin4Name_ua",'survey_total','class')]
mapview::mapview(Rect_Grid)
Rect_Grid <- Rect_Grid %>%
group_by(admin4Name_ua,admin4Pcode) %>%
mutate(n=1,
Name = paste(admin4Name_ua, " - ", rect_id, " - ",cumsum(n)," із ", survey_total,
"(main:",Survey,", buffer:",survey_total-Survey,")",
sep = "")) %>%
ungroup() %>% dplyr::select(-n)
Rect_Grid
View(Rect_Grid)
Rect_Grid_Line <- ms_explode(Rect_Grid[,1])
Rect_Grid_Line <- st_cast(Rect_Grid_Line,"LINESTRING")
Rect_Grid_Points <- st_point_on_surface(Rect_Grid[,10])
Rect_Grid_Points <- st_point_on_surface(Rect_Grid[,8])
